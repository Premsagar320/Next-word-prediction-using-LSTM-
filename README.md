# Next-word-prediction-using-LSTM-
Built a character-level LSTM model from scratch, implementing the input, forget, and output gates manually along with backpropagation through time and a custom Adam optimizer. Trained the model on the Sherlock Holmes text corpus to understand sequence modeling, long-range dependencies, and how gradients flow through recurrent networks.
